# Scraper Platform v5.0 ğŸš€

> **Unified pipeline architecture for scalable, maintainable web scraping**

## What's New in v5.0

**Major architectural consolidation** - We've unified 3 separate pipeline systems into one coherent framework:

- âœ… **Single execution model** - One `PipelineRunner` replaces multiple orchestrators
- âœ… **Standardized DAGs** - All scrapers use same 12-line pattern
- âœ… **Parallel execution** - Automatic parallelization of independent steps
- âœ… **Type-safe pipeline steps** - Clear step types (FETCH, PARSE, TRANSFORM, VALIDATE, ENRICH, EXPORT)
- âœ… **Comprehensive docs** - Full migration guide and examples

## Quick Links

| Document | Description |
|----------|-------------|
| [ğŸ“– Quick Start](QUICK_START_V5.md) | Create a scraper in 5 minutes |
| [ğŸ—ï¸ Architecture](ARCHITECTURE_V5.md) | Deep dive into v5 architecture |
| [ğŸ“‹ Cleanup Summary](CLEANUP_SUMMARY.md) | What changed and what's next |

## Installation

```bash
# Clone repository
git clone https://github.com/your-org/scraper-platform
cd scraper-platform

# Install dependencies
pip install -r requirements.txt

# Set up environment
cp .env.example .env
# Edit .env with your settings

# Run migrations
./scripts/migrate.sh

# Start Airflow (optional)
docker-compose up -d
```

## Creating Your First Scraper

### 1. Generate Scaffold

```bash
python tools/add_scraper_advanced.py my_scraper \
  --engine selenium \
  --base-url https://example.com \
  --interactive
```

### 2. Implement Functions

Edit `src/scrapers/my_scraper/pipeline.py`:

```python
def fetch_listings(**params):
    """Fetch product URLs."""
    return ["url1", "url2", "url3"]

def parse_products(fetch_listings, **params):
    """Parse product details."""
    urls = fetch_listings
    return [{"name": "Product", "price": "$10"}]

def export_database(parse_products, **params):
    """Save to database."""
    products = parse_products
    # Save to DB
    return {"item_count": len(products)}
```

### 3. Define Pipeline

Edit `dsl/pipelines/my_scraper.yaml`:

```yaml
pipeline:
  name: my_scraper
  steps:
    - id: fetch
      component: my_scraper.fetch_listings
      type: fetch
    - id: parse
      component: my_scraper.parse_products
      type: parse
      depends_on: [fetch]
    - id: export
      component: my_scraper.export_database
      type: export
      depends_on: [parse]
```

### 4. Run It

```bash
python -m src.entrypoints.run_pipeline \
  --source my_scraper \
  --environment dev
```

**That's it!** Your scraper is ready. âœ¨

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Airflow DAG                           â”‚
â”‚              (build_scraper_dag factory)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PipelineRunner                             â”‚
â”‚  â€¢ Dependency resolution                                     â”‚
â”‚  â€¢ Parallel execution (non-export steps)                     â”‚
â”‚  â€¢ Sequential execution (export steps)                       â”‚
â”‚  â€¢ Error handling & retries                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 PipelineCompiler                             â”‚
â”‚  â€¢ Loads YAML pipeline definition                            â”‚
â”‚  â€¢ Resolves components from UnifiedRegistry                  â”‚
â”‚  â€¢ Validates dependencies                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               CompiledPipeline                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  FETCH   â”‚â†’ â”‚  PARSE   â”‚â†’ â”‚ ENRICH   â”‚â†’ â”‚  EXPORT  â”‚   â”‚
â”‚  â”‚  Step    â”‚  â”‚  Step    â”‚  â”‚  Step    â”‚  â”‚  Step    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Features

### ğŸš€ Parallel Execution
Non-export steps run in parallel when dependencies are satisfied:

```yaml
steps:
  - id: fetch_category_a
    type: fetch
  - id: fetch_category_b
    type: fetch
  # â†‘ These run in parallel
  
  - id: merge
    depends_on: [fetch_category_a, fetch_category_b]
```

### ğŸ”„ Automatic Retries
Built-in retry logic with exponential backoff:

```yaml
steps:
  - id: fetch_data
    retry: 3  # Retry up to 3 times
    timeout: 60  # 60 second timeout
```

### âš ï¸ Optional Steps
Steps that won't fail the pipeline:

```yaml
steps:
  - id: optional_validation
    required: false  # Pipeline continues if this fails
```

### ğŸ“Š Comprehensive Tracking
Every step tracked with timing, status, and output:

```python
result = runner.run(pipeline, source="alfabeta")
# result.step_results = {
#   "fetch": StepResult(status="success", duration=2.3, ...),
#   "parse": StepResult(status="success", duration=1.1, ...),
#   ...
# }
```

## Project Structure

```
scraper-platform/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline/              # â­ NEW: Unified pipeline system
â”‚   â”‚   â”œâ”€â”€ runner.py          # PipelineRunner
â”‚   â”‚   â”œâ”€â”€ registry.py        # UnifiedRegistry
â”‚   â”‚   â”œâ”€â”€ compiler.py        # PipelineCompiler
â”‚   â”‚   â””â”€â”€ step.py            # PipelineStep
â”‚   â”œâ”€â”€ scrapers/              # Source-specific scrapers
â”‚   â”‚   â”œâ”€â”€ alfabeta/
â”‚   â”‚   â”œâ”€â”€ argentina/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ engines/               # HTTP, Browser engines
â”‚   â”œâ”€â”€ processors/            # QC, dedupe, normalize, PCID
â”‚   â”œâ”€â”€ exporters/             # DB, S3, GCS exporters
â”‚   â””â”€â”€ entrypoints/           # CLI and programmatic entry
â”‚
â”œâ”€â”€ dags/                      # Airflow DAG definitions
â”‚   â”œâ”€â”€ scraper_base.py        # â­ UPDATED: DAG factory
â”‚   â”œâ”€â”€ scraper_alfabeta_v5.py # â­ NEW: V5 DAGs
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ dsl/                       # Pipeline definitions
â”‚   â”œâ”€â”€ components.yaml        # Component registry
â”‚   â””â”€â”€ pipelines/             # Pipeline YAML files
â”‚
â”œâ”€â”€ config/                    # Configuration
â”‚   â”œâ”€â”€ settings.yaml
â”‚   â”œâ”€â”€ env/                   # Environment configs
â”‚   â””â”€â”€ sources/               # Source-specific configs
â”‚
â”œâ”€â”€ tests/                     # Test suite
â”‚
â”œâ”€â”€ ARCHITECTURE_V5.md         # â­ NEW: Architecture docs
â”œâ”€â”€ QUICK_START_V5.md          # â­ NEW: Quick start guide
â”œâ”€â”€ CLEANUP_SUMMARY.md         # â­ NEW: Cleanup summary
â””â”€â”€ README_V5.md               # â­ This file
```

## Migration from v4.x

### For Existing Scrapers

Your v4 scrapers **still work**! No breaking changes. But to get v5 benefits:

1. Create pipeline definition: `dsl/pipelines/{source}.yaml`
2. Register components: `dsl/components.yaml`
3. Create v5 DAG using `build_scraper_dag()`
4. Test in dev environment
5. Deploy when ready

**See**: [Architecture Guide](ARCHITECTURE_V5.md) for detailed migration steps.

### For New Scrapers

Use the scaffolding tool - it generates v5-ready code:

```bash
python tools/add_scraper_advanced.py new_source --engine selenium
```

## Running Scrapers

### Command Line

```bash
# Dev environment
python -m src.entrypoints.run_pipeline \
  --source alfabeta \
  --environment dev

# Production with custom parameters
python -m src.entrypoints.run_pipeline \
  --source alfabeta \
  --environment prod \
  --run-type FULL_REFRESH \
  --params '{"max_pages": 50}'
```

### Airflow UI

1. Open Airflow: `http://localhost:8080`
2. Find DAG: `scraper_alfabeta_v5`
3. Trigger with config:
   ```json
   {
     "run_type": "FULL_REFRESH",
     "environment": "prod"
   }
   ```

### Jira Integration

Tag Jira issue with:
- `scraper:alfabeta`
- `run_type:FULL_REFRESH`
- `environment:prod`

DAG triggers automatically via webhook.

## Configuration

### Source Configuration
`config/sources/alfabeta.yaml`
```yaml
base_url: https://alfabeta.com
login_url: https://alfabeta.com/login
require_login: true

selectors:
  product_title: ".product-name"
  price: ".price"
```

### Environment Configuration  
`config/env/prod.yaml`
```yaml
database:
  host: prod-db.example.com
  port: 5432

s3:
  bucket: scraper-prod-bucket
```

## Development

### Running Tests

```bash
# All tests
pytest

# Specific test file
pytest tests/test_pipeline.py

# With coverage
pytest --cov=src tests/
```

### Code Quality

```bash
# Linting
ruff check src/

# Type checking
mypy src/

# Formatting
black src/
```

### Local Development

```bash
# Start services
docker-compose up -d

# Watch logs
docker-compose logs -f airflow-scheduler

# Stop services
docker-compose down
```

## Deployment

### Docker

```bash
# Build image
docker build -t scraper-platform:v5.0 .

# Run
docker run -p 8080:8080 scraper-platform:v5.0
```

### Kubernetes

```bash
# Deploy
kubectl apply -f k8s/

# Check pods
kubectl get pods -n scraper-platform

# Logs
kubectl logs -f deployment/scraper-airflow -n scraper-platform
```

## Monitoring

### Metrics
- **Run tracking**: `scraper_runs` table
- **Step timing**: Per-step execution times
- **Success rates**: By source and environment
- **Item counts**: Products scraped per run

### Logs
- **Airflow**: Task logs in Airflow UI
- **Application**: Structured JSON logs
- **Database**: Query logs and performance

### Alerts
- Failed runs trigger alerts
- Low output counts detected
- Proxy failures monitored
- Database connection issues

## Troubleshooting

| Problem | Solution |
|---------|----------|
| Pipeline not found | Create `dsl/pipelines/{source}.yaml` |
| Component not registered | Add to `dsl/components.yaml` |
| Import error | Verify module path and function name |
| Dependency error | Check `depends_on` references valid IDs |
| Export failure | Check database connection and permissions |

**See**: [Quick Start Guide](QUICK_START_V5.md) for detailed troubleshooting.

## Performance

### Benchmarks (v5 vs v4)

| Metric | v4.x | v5.0 | Improvement |
|--------|------|------|-------------|
| DAG code | 120 lines | 12 lines | **-90%** |
| Pipeline systems | 3 | 1 | **-66%** |
| Avg run time | 45s | 32s | **-29%** |
| Parallel steps | No | Yes | **+âˆ** |

### Optimization Tips

1. **Increase workers**: `PipelineRunner(max_workers=8)`
2. **Batch exports**: Bulk insert instead of one-by-one
3. **Cache lookups**: Use `@lru_cache` for repeated queries
4. **Optimize selectors**: Use IDs over class names when possible

## Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request

## License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

## Support

- **Documentation**: [ARCHITECTURE_V5.md](ARCHITECTURE_V5.md), [QUICK_START_V5.md](QUICK_START_V5.md)
- **Examples**: Check `src/scrapers/` for reference implementations
- **Issues**: Create GitHub issue with logs and pipeline YAML
- **Questions**: Open discussion in GitHub Discussions

---

**Version**: 5.0.0  
**Status**: âœ… Production Ready  
**Last Updated**: 2024-11-27

Made with â¤ï¸ by the Scraper Platform Team
